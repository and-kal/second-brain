---
title: "Algorithms of thought"
date: "2023-01-05"
draft: true
---

One can only argue over cognitive algorithms, not about the reality of things.
That is, when thinking about things in the world or describing those things, one can always wonder, if one's description really encompasses all the facts there are to this thing or if there's something hidden, which would challenge our conception of it. Now the thing is, that we don't have access to the true nature of things, but only to our conception of the thing (what we know about the thing and how these bits of knowledge are connected) and we'll have to the check if that conception is consistent and "whether [that] categorization should be active in their neural networks" as Eliezer Yudkowsky wrote in [How An Algorithm Feels From Inside](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside).

Scott Alexander: [The Categories Were Made For Man, Not Man For The Categories](https://www.lesswrong.com/posts/aMHq4mA2PHSM2TMoH/the-categories-were-made-for-man-not-man-for-the-categories)

semantic arguments are aimed at confirming to the usage of words
(is a whale a fish or a mammal)

categories come with criteria

categories are constructed for certain goals
-> critique must be aimed at these goals, when it's not aimed at proving that something is wrong in the sense of "factually incorrect"
-> negotiable and non-negotiable facts

there can be alternative categorization system

ambiguity and disagreements can be resolved by agreeing on a common authority (e.g. science) as a 'source of truth'

NOS = not otherwise specified
-> objects that fit the category-criteria somewhat, but not completely

<!-- https://www.lesswrong.com/posts/895quRDaK6gR2rM82/diseased-thinking-dissolving-questions-about-disease -->
<!-- https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace -->

> And these LLMs feature no symbolic reasoning whatsoever within their computational substrate. What they do feature is a simple recursive model: Given the input so far, what is the next token? And they are thus enabled after training on huge amounts of input material. No inherent reasoning capabilities, no primordial ability to apply logic, or even infer basic axioms of logic, reasoning, thought. And therefore unrecognizable to Chomsky's definition. ([papaver-somnamb on HackerNews](https://news.ycombinator.com/item?id=44093563))
<!-- https://chomsky.info/20230503-2/ -->